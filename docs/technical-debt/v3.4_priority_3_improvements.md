# v3.4 - Remaining Data Utilization Improvements (Priority 3)

## Status
Priority 1+2 from v3.3 Phase B++ complete (**83% utilization achieved**).

Remaining improvements for **98% target utilization**.

---

## Background

v3.3 Phase B++ successfully implemented Priority 1+2 improvements:
- ✅ **Priority 1** (45 min, +5% utilization): Document types, image legal relevance, object patterns
- ✅ **Priority 2** (1.5 hours, +7% utilization): Document summaries, temporal patterns, timeline gaps

**Current state**: 83% data utilization (up from 71%)
**Target**: 98% utilization

---

## Priority 3: Remaining High-Value Improvements

### 1. Package Word Cloud/Frequency Visualizations (60 min)
**Current Problem**: Word clouds and frequency charts are generated for every document and saved to `data/storage/derived/sha256=.../word_cloud.png`, but they're NOT included in the client deliverable ZIP.

**Impact**: We're paying AI to analyze word frequency patterns, generating visualizations, but clients never see them.

**Implementation**:
- **File**: `src/evidence_toolkit/pipeline/package.py` (PackageGenerator)
- **Change**: Copy `word_cloud.png` and `word_frequency.png` from each evidence's derived directory into `package/visualizations/`
- **Pattern**: Similar to how we already copy analysis JSON files

**Code Location**:
```python
# In PackageGenerator._create_zip_package()
# After copying evidence files, copy visualizations:
for evidence in evidence_summaries:
    derived_dir = storage.derived_dir / f"sha256={evidence.sha256}"
    for viz_file in ['word_cloud.png', 'word_frequency.png']:
        src = derived_dir / viz_file
        if src.exists():
            dest = viz_dir / f"{evidence.filename}_{viz_file}"
            shutil.copy2(src, dest)
```

**Benefit**: Clients see visual analysis patterns for each document (word clouds show key terms visually).

---

### 2. Show Sentiment Per Document (40 min)
**Current Problem**: AI analyzes sentiment for every document (`neutral`, `hostile`, `professional`, etc.) but we only display it for email threads, not individual documents.

**Impact**: Users see "Document analyzed" but don't know if it was hostile, professional, emotional, etc.

**Implementation**:
- **File**: `src/evidence_toolkit/pipeline/summary.py` line 1689 (evidence analysis display)
- **Change**: Add sentiment indicator after document type
- **Current**: `1. manager_memo.txt (EMAIL)`
- **New**: `1. manager_memo.txt (EMAIL - HOSTILE)`

**Code**:
```python
# Line 1689 in summary.py
type_display = summary.document_type.upper() if summary.document_type else summary.evidence_type.upper()

# Add sentiment if available
if summary.evidence_type == 'document':
    analysis_file = self.storage.derived_dir / f"sha256={summary.sha256}" / "analysis.v1.json"
    # Load and extract sentiment
    sentiment = doc_analysis.get('sentiment', '').upper()
    if sentiment:
        type_display += f" - {sentiment}"
```

**Benefit**: Context for document tone - "Hostile memo" vs "Professional letter" tells a story.

---

### 3. Display Entity Context in Correlation Table (45 min)
**Current Problem**: Entity correlation table shows:
```
• Paul Boucherat (person)
  Appears in 2 evidence pieces
  Average confidence: 0.97
```

But we're not showing WHERE Paul appeared (the context field that OpenAI extracted).

**Impact**: Users see entity counts but not sample contexts like "From: Paul Boucherat" or "witnessed by Paul B."

**Implementation**:
- **File**: `src/evidence_toolkit/pipeline/summary.py` line 1750 (entity correlations display)
- **Change**: Add sample context for each entity
- **New format**:
```
• Paul Boucherat (person)
  Appears in 2 evidence pieces
  Average confidence: 0.97
  Sample contexts:
    - "From: Paul Boucherat" (manager_memo.txt)
    - "I witnessed the incident on January 12th" (witness_paul.txt)
```

**Code**:
```python
# Line 1750 in summary.py
for corr in case_summary.correlation_result.entity_correlations:
    lines.append(f"• {corr.entity_name} ({corr.entity_type})")
    lines.append(f"  Appears in {corr.occurrence_count} evidence pieces")
    lines.append(f"  Average confidence: {corr.confidence_average:.2f}")

    # NEW: Show sample contexts
    contexts = []
    for occ in corr.evidence_occurrences[:2]:  # First 2 contexts
        context_str = occ.get('context', '')[:60]
        if context_str:
            contexts.append(f'    - "{context_str}..."')
    if contexts:
        lines.append("  Sample contexts:")
        lines.extend(contexts)
```

**Benefit**: User sees HOW entities appeared in evidence, not just that they appeared.

---

## Expected Results

### Before (v3.3 Phase B+)
- **83% data utilization**
- Word clouds generated but not shown
- Document sentiment captured but not displayed
- Entity contexts captured but hidden

### After (v3.4)
- **98% data utilization**
- Visual analysis in client deliverables
- Document tone indicators ("Hostile memo")
- Entity contexts showing usage patterns

---

## Files Modified
1. `src/evidence_toolkit/pipeline/package.py` (~30 lines)
2. `src/evidence_toolkit/pipeline/summary.py` (~60 lines)

---

## Effort Estimate
- Priority 3.1 (Word clouds): 60 minutes
- Priority 3.2 (Sentiment): 40 minutes
- Priority 3.3 (Entity context): 45 minutes

**Total: 2.5 hours → +15% utilization → 98% target**

---

## Testing
```bash
# Re-run test case with Priority 3 changes
uv run evidence-toolkit process-case data/cases/test_full_001 --case-id test004 --case-type workplace

# Check package output
unzip -l data/packages/test004_analysis_package_*.zip

# Verify:
# ✓ visualizations/ directory contains word clouds
# ✓ Document sentiments shown in evidence list
# ✓ Entity contexts displayed in correlation table
```

---

## Labels
`enhancement`, `data-utilization`, `v3.4`

---

## Related
- v3.3 Phase B++ (Priority 1+2): Implemented in this session
- Data Utilization Audit: `docs/technical-debt/v3.3_data_utilization_audit.md`
- GitHub Issue: Create issue from this document content

---

**Document created**: 2025-10-09
**Status**: Ready for GitHub issue creation
